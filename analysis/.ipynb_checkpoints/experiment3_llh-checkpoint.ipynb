{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of the packages\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "from graph_tool.all import *\n",
    "import random\n",
    "import graph_tool.topology as gt\n",
    "import graph_tool.clustering as gc\n",
    "import graph_tool.centrality as gcent\n",
    "import graph_tool.generation as gg\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "import copy\n",
    "from matplotlib.lines import Line2D \n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from collections import Counter\n",
    "import math\n",
    "import scipy.stats\n",
    "import collections\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm \n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn.utils.fixes import parse_version \n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from collections import Counter\n",
    "#from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats.stats import pearsonr \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import os\n",
    "#import shap\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d5715",
   "metadata": {},
   "source": [
    "# A-Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805af0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition_synthetic_(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which returns 1 if the condition to be infected by the complex contagion is reached, 0 otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    traj_nei = x.trajectory_neighbors\n",
    "    traj_node = x.trajectory_node\n",
    "\n",
    "    time_contagion_nei = [np.where(k == 1)[0][0] for k in traj_nei if (1 in k)]\n",
    "    time_contagion_node = np.where(traj_node == 1)[0][0]\n",
    "\n",
    "    if time_contagion_node-1 in time_contagion_nei:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "def likelihood_sp(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Likelihood to be infected by the spontaneous adoption\n",
    "    \"\"\"\n",
    "    \n",
    "    waiting_time = list(x.trajectory_node).index(1)-1\n",
    "    return r*(1-r)**(waiting_time)\n",
    "\n",
    "def likelihood_si(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Likelihood to be infected by the simple adoption\n",
    "    \"\"\"\n",
    "    \n",
    "    waiting_time = list(x.trajectory_node).index(1)-1\n",
    "    sum_stimuli = x.sum_stimuli\n",
    "    \n",
    "    if sum_stimuli != 0:\n",
    "        beta = 1/sum_stimuli\n",
    "    else:\n",
    "        beta = 0\n",
    "\n",
    "    return beta*(1-beta)**(sum_stimuli-1)*(1-r)**(waiting_time)\n",
    "\n",
    "def likelihood_cp(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Likelihood to be infected by the complex adoption\n",
    "    \"\"\"\n",
    "    \n",
    "    condition = x.condition\n",
    "    waiting_time = list(x.trajectory_node).index(1)-1\n",
    "    \n",
    "    return condition * (1-r)**(waiting_time)\n",
    "\n",
    "def classification(x):\n",
    "\n",
    "    \"\"\"\n",
    "    Returning the inferred classification, 0 for simple, 1 for complex and 2 if it is spontaneous\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    llh_sp = x.likelihood_sp\n",
    "    llh_si = x.likelihood_si\n",
    "    llh_cp = x.likelihood_cp\n",
    "    \n",
    "    if (llh_si>llh_sp) & (llh_si>llh_cp):\n",
    "        return(0)\n",
    "    \n",
    "    elif (llh_cp>llh_si) & (llh_cp>llh_sp):\n",
    "        return(1)\n",
    "    \n",
    "    elif (llh_sp>llh_si) & (llh_sp>llh_cp):\n",
    "        return(2)\n",
    "    \n",
    "    elif (llh_si == llh_cp) & (llh_si>llh_sp): \n",
    "\n",
    "        if random.random()>=0.5:\n",
    "            return(0)\n",
    "        else:\n",
    "            return(1)\n",
    "        \n",
    "    elif (llh_si == llh_sp) & (llh_si>llh_cp):\n",
    "        if random.random()>=0.5:\n",
    "            return(0)\n",
    "        else:\n",
    "            return(2)\n",
    "        \n",
    "    elif (llh_cp == llh_sp) & (llh_cp>llh_si):\n",
    "        if random.random()>=0.5:\n",
    "            return(1)\n",
    "        else:\n",
    "            return(2)\n",
    "    \n",
    "    else:\n",
    "        if random.random()<=(1/3):\n",
    "            return(0)\n",
    "        elif random.random()<=(2/3):\n",
    "            return(1)\n",
    "        else:\n",
    "            return(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835537a",
   "metadata": {},
   "source": [
    "# B- getting the path of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47774f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dir_data = os.getcwd()[:-8]+'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbb0ea",
   "metadata": {},
   "source": [
    "# C- making the classification on the whole dataset of Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5332bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cascade = 'general'\n",
    "sample_size = 2000\n",
    "nber_sample = 10\n",
    "list_beta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "list_phi = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f93319",
   "metadata": {},
   "outputs": [],
   "source": [
    "for network in ['ER', 'BA', 'WS', 'SBM', 'twitter']: # 'BA', 'WS', 'SBM'\n",
    "    \n",
    "    print('NETWORK', network)\n",
    "\n",
    "    matrix_accuracies = np.zeros((len(list_beta), len(list_phi)))\n",
    "    matrix_confusion_matrices = np.asarray([[np.array([[0,0,0],[0,0,0],[0,0,0]]) for k in range(len(list_phi))] for i in range(len(list_beta))])\n",
    "\n",
    "    for index_beta, beta in enumerate(list_beta):\n",
    "        for index_phi, phi in enumerate(list_phi):\n",
    "            \n",
    "            list_accuracies = []\n",
    "            list_confusion_matrices = []\n",
    "            \n",
    "            for it in range(nber_sample):\n",
    "\n",
    "                print('beta', beta, 'phi', phi, 'it', it)\n",
    "                \n",
    "                #############################################\n",
    "\n",
    "                # loading of the data base\n",
    "                with open(x_dir_data+'/df_experiment2_3/df_'+network+'_premix_different_parameter.pickle', 'rb') as handle:\n",
    "                    df_general = pickle.load(handle)\n",
    "\n",
    "                df_SI = df_general[(df_general['contagion'] == 0) & (df_general['parameter'] == beta)]\n",
    "                df_CP = df_general[(df_general['contagion'] == 1) & (df_general['parameter'] == phi)]\n",
    "\n",
    "                # random infection written as 2 in the contagion column\n",
    "                df_SI['contagion'] = np.where(df_SI.is_seed == 2,2, df_SI['contagion'])\n",
    "                df_CP['contagion'] = np.where(df_CP.is_seed == 2,2, df_CP['contagion'])\n",
    "\n",
    "                # seeds written as 2 in the contagion column\n",
    "                df_SI['contagion'] = np.where(df_SI.is_seed == 1,2, df_SI['contagion'])\n",
    "                df_CP['contagion'] = np.where(df_CP.is_seed == 1,2, df_CP['contagion'])\n",
    "                \n",
    "                \n",
    "                df_random = pd.concat([df_SI[df_SI['contagion'] == 2],df_CP[df_CP['contagion'] == 2]])\n",
    "                df_SI = df_SI[df_SI['contagion'] == 0]\n",
    "                df_CP = df_CP[df_CP['contagion'] == 1]\n",
    "\n",
    "                # bootstap, we select sample_size\n",
    "                df_sample_SI = df_SI.sample(n=int(sample_size), replace=True, axis=0) \n",
    "                df_sample_CP = df_CP.sample(n=int(sample_size), replace=True, axis=0)\n",
    "                df_sample_random = df_random.sample(n=int(sample_size), replace=True, axis=0)\n",
    "\n",
    "                df = pd.concat([df_sample_SI,df_sample_CP,df_sample_random])\n",
    "                \n",
    "                df['condition'] = df.apply(make_condition_synthetic_, axis = 1)\n",
    "                \n",
    "                r = np.sum(df['nber_infected_neighbors'] == 0) / np.sum([list(df.iloc[k]['trajectory_node']).count(0) for k in range(len(df))])\n",
    "\n",
    "                df['likelihood_sp'] = df.apply(likelihood_sp, axis = 1)\n",
    "                df['likelihood_si'] = df.apply(likelihood_si, axis = 1)\n",
    "                df['likelihood_cp'] = df.apply(likelihood_cp, axis = 1)\n",
    "                \n",
    "                df['classification'] = df.apply(classification, axis = 1)\n",
    "                \n",
    "                df.dropna(subset=['classification'], inplace=True)\n",
    "                \n",
    "                conf = confusion_matrix(list(df['contagion']), list(df['classification']))\n",
    "                conf = conf.T\n",
    "                \n",
    "                #############################################\n",
    "\n",
    "                accuracy = (conf[0,0] + conf[1,1] + conf[2,2]) / np.sum(conf)\n",
    "                \n",
    "                list_accuracies.append(accuracy)\n",
    "                list_confusion_matrices.append(conf)\n",
    "                \n",
    "                ####################################################\n",
    "\n",
    "            matrix_accuracies[index_beta, index_phi] = np.mean(list_accuracies)\n",
    "            matrix_confusion_matrices[index_beta][index_phi] = np.mean(list_confusion_matrices,axis=0)\n",
    "\n",
    "    with open('_llh_experiment2_scores_matrix_'+network+'_premix_different_parameter.pickle', 'wb') as handle:\n",
    "        pickle.dump(matrix_accuracies, handle)\n",
    "\n",
    "    with open('_llh_experiment2_confusion_matrices_'+network+'_premix_different_parameter.pickle', 'wb') as handle:\n",
    "        pickle.dump(matrix_confusion_matrices, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
