{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47262045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of the packages\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "from graph_tool.all import *\n",
    "import random\n",
    "import graph_tool.topology as gt\n",
    "import graph_tool.clustering as gc\n",
    "import graph_tool.centrality as gcent\n",
    "import graph_tool.generation as gg\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "import copy\n",
    "from matplotlib.lines import Line2D \n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from collections import Counter\n",
    "import math\n",
    "import scipy.stats\n",
    "import collections\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm \n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn.utils.fixes import parse_version \n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from collections import Counter\n",
    "#from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats.stats import pearsonr \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import os\n",
    "#import shap\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81ce58",
   "metadata": {},
   "source": [
    "# A- getting the path of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f96bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dir_data = os.getcwd()[:-8]+'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71208a",
   "metadata": {},
   "source": [
    "# B-Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3921c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_model(name_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which takes in input the name of a machine learning algorithm and returns the model and the parameters for the grid search\n",
    "    \"\"\"\n",
    "    \n",
    "    if (name_model == 'SVM'):\n",
    "        \n",
    "        # C is the strength of the regularization\n",
    "        # kernel must be linear\n",
    "        # shrinking: shorten the training time if optimization problem solved/default is True\n",
    "        # probability: enable probability estimate, default is False\n",
    "        # tol: tolerance for stopping default=1e-3\n",
    "        # cache_size: size max before stopping default=200\n",
    "        # put weight on classes / default = False\n",
    "        # verbosebool: Enable verbose output - may conduct to errors/ default=False\n",
    "        # max_iter max of iteration, default=-1 meaning that infinite\n",
    "\n",
    "        model = SVC(kernel='linear')\n",
    "\n",
    "        # choose the best parameters\n",
    "\n",
    "        #Documentation on the C parameter:\n",
    "        #Parameter which \"soften\" the margin: that is, it allows some of the points to creep \n",
    "        #into the margin if that allows a better fit. The hardness of the margin is controlled \n",
    "        #by a tuning parameter, most often known as ğ¶. For very large ğ¶, the margin is hard, and \n",
    "        #points cannot lie in it. For smaller ğ¶, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "        #param_grid = {'svc__C': [0.01,0.1,1, 10,100,1000,10000]}\n",
    "        param_grid = {'C': [0.001,0.01,0.1,1, 10,100,1000,10000,100000]}\n",
    "        \n",
    "    elif (name_model == 'decision_tree'):\n",
    "        \n",
    "        # criterion is gini or entropy - Default = gini\n",
    "        # splitter split the best or the random best - â€œbestâ€, â€œrandomâ€ - Default = \"best\"\n",
    "        # max_depth of the tree - Default=None (no limit)\n",
    "        # min_samples_split - minimum number of samples required to split an internal node - Default=2\n",
    "        # min_sample_leaf - minimum number of samples required to be at a leaf node - Default=1\n",
    "        # min_weight_fraction_leaf for weighted samples\n",
    "        # max_features - maximum number of features to take into account - Default = None (-all the features)\n",
    "        # random_state\n",
    "        # max_leaf_node - number of leaf nodes - Default = None (no limit)\n",
    "        # min_impurity_decrease - split induces a decrease of the impurity greater than or equal to this value - Default = 0\n",
    "        # class_weight - for weighted data\n",
    "        # ccp_alpha - The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen - Default = 0\n",
    "        \n",
    "        model = DecisionTreeClassifier(splitter='best', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                       random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       class_weight=None, ccp_alpha=0.0)\n",
    "        \n",
    "        param_grid = {'decisiontreeclassifier__criterion':['gini', 'entropy']}\n",
    "        \n",
    "    elif (name_model == 'random_forest'):\n",
    "        \n",
    "        # n_estimators - number of trees - default is 100\n",
    "        # bootstrap - if we use all the dataset to do a tree - default = True\n",
    "        # oob_score - if bootstrap = True\n",
    "        # n_jobs - run in parallele\n",
    "        # warm_start - use the paraemter of the last run as a start\n",
    "        # max_samples - if bootstrap is true\n",
    "        \n",
    "        model = RandomForestClassifier(bootstrap = True)#n_estimators=100, max_depth=None, \n",
    "                                       #min_samples_split=2, min_samples_leaf=1, \n",
    "                                       #min_weight_fraction_leaf=0.0, #   max_features=None, \n",
    "                                       #max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       #oob_score=False, # bootstrap=False, \n",
    "                                       #n_jobs=None, random_state=None,                                     \n",
    "                                       #verbose=0, warm_start=False, \n",
    "                                       #class_weight=None, ccp_alpha=0.0, \n",
    "                                       #max_samples=None)\n",
    "        \n",
    "        param_grid = {'randomforestclassifier__criterion':['gini', 'entropy']}\n",
    "        \n",
    "    elif (name_model == 'knn'):\n",
    "        \n",
    "        # n_neighbors - Number of neighbors to considere - Default = 5\n",
    "        # weights 'uniform' or 'distance' - weight points in function of the distance or uniformly - Default = uniform\n",
    "        # kind of used algo - Default = 'auto' - try to decide the best\n",
    "        # leaf_size - to BallTree or KDTree - about the speed of the construction - Default = 30\n",
    "        # p - Power parameter for the Minkowski metric - p=2 is euclidean distance\n",
    "        # metric - of the distance 'minkowski'\n",
    "        # metric_params-Additional keyword arguments for the metric function\n",
    "        # n_jobs - The number of parallel jobs to run for neighbors search. None means 1\n",
    "        \n",
    "        model = KNeighborsClassifier(n_neighbors=5, weights='uniform', n_jobs=1)\n",
    "        \n",
    "        param_grid = {'kneighborsclassifier__n_neighbors':[5,10,15,20],'kneighborsclassifier__weights':['uniform','distance']}\n",
    "        \n",
    "    elif (name_model == 'naive_bayes'):\n",
    "        \n",
    "        # priors - Prior probabilities of the classes. If specified the priors are not adjusted according to the data.Default = None\n",
    "        # var_smoothing Portion of the largest variance of all features \n",
    "        #that is added to variances for calculation stability - Default = 1e-9\n",
    "        \n",
    "        model = GaussianNB(priors=None, var_smoothing=1e-9)\n",
    "        \n",
    "        param_grid = {'gaussiannb__var_smoothing':[1e-10,1e-9,1e-8]}\n",
    "        \n",
    "    elif (name_model == 'perceptron'):\n",
    "        \n",
    "        # penalty: gives a penalty to avoid overfitting in {â€˜l2â€™,â€™l1â€™,â€™elasticnetâ€™}, default=None\n",
    "        # alpha - cst which multiplies the penalty: default=0.0001\n",
    "        # l1_ratio -only for penalty='elasticnet' - Default=0.15\n",
    "        # fit_interceptbool, default=True - Whether the intercept should be estimated or not.If False, the data is assumed to be already centered.\n",
    "        # max_iterint, maximum number of passes over the training data (aka epochs) - default=1000\n",
    "        # tol float, default=1e-3 Stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).\n",
    "        # shufflebool, default=True - Whether or not the training data should be shuffled after each epoch.\n",
    "        # verboseint, default=0 - The verbosity level.\n",
    "        # eta0 double, default=1 Constant by which the updates are multiplied.\n",
    "        # n_jobsint, default=None - The number of CPUs to use to do the OVA (One Versus All)\n",
    "        # random_stateint, RandomState instance, default=None\n",
    "        # early_stopping bool, default=False-Whether to use early stopping to terminate training when validation\n",
    "        # validation_fraction float, default=0.1 The proportion of training data to set aside as validation set for early stopping.\n",
    "        # n_iter_no_change int, default=5 - Number of iterations with no improvement to wait before early stopping.\n",
    "        # class_weightdict, {class_label: weight} or â€œbalancedâ€, default=None - Preset for the class_weight fit parameter.\n",
    "            #Weights associated with classes. If not given, all classes are supposed to have weight one.\n",
    "            #The â€œbalancedâ€ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "        # warm_startbool, default=False - when set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.\n",
    "\n",
    "        \n",
    "        model = Perceptron(penalty=None, max_iter = 20000)\n",
    "        \n",
    "        param_grid = {'perceptron__penalty':[None, 'l2', 'l1', 'elasticnet'],'perceptron__alpha':[0.00001,0.0001,0.001],'perceptron__eta0':[0.5,1,2,3,4,5]}\n",
    "        \n",
    "    elif (name_model == 'linear_svm'):\n",
    "        \n",
    "        \n",
    "        #penalty{â€˜l1â€™, â€˜l2â€™}, default=â€™l2â€™ - Specifies the norm used in the penalization.\n",
    "        #loss{â€˜hingeâ€™, â€˜squared_hingeâ€™}, default=â€™squared_hingeâ€™-Specifies the loss function. \n",
    "        #â€˜hingeâ€™ is the standard SVM loss (used e.g. by the SVC class) while â€˜squared_hingeâ€™ is the square of the hinge loss. The combination of penalty='l1' and loss='hinge' is not supported.\n",
    "        # dualbool, default=True - Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        # tolfloat, default=1e-4 - Tolerance for stopping criteria. - Cfloat, default=1.0\n",
    "        # C: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "        # multi_class{â€˜ovrâ€™, â€˜crammer_singerâ€™}, default=â€™ovrâ€™ - Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored.\n",
    "        # fit_interceptbool, default=True - To center the data. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered).\n",
    "        # intercept_scaling float, default=1 - When self.fit_intercept is True, \n",
    "            #instance vector x becomes [x, self.intercept_scaling], i.e. a â€œsyntheticâ€ feature with constant value\n",
    "            #equals to intercept_scaling is appended to the instance vector. \n",
    "            #The intercept becomes intercept_scaling * synthetic feature weight \n",
    "            #Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. \n",
    "            #To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept)\n",
    "            #intercept_scaling has to be increased.\n",
    "\n",
    "        # class_weightdict or â€˜balancedâ€™, default=None - Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. \n",
    "        # The â€œbalancedâ€ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "        # verboseint, default=0 - Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.\n",
    "        # random_stateint, RandomState instance or None, default=None - Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if dual=True). When dual=False the underlying implementation of LinearSVC is not random and random_state has no effect on the results. Pass an int for reproducible output across multiple function calls. See Glossary.\n",
    "        # max_iterint, default=1000 - The maximum number of iterations to be run.\n",
    "\n",
    "\n",
    "        \n",
    "        model = LinearSVC(loss='squared_hinge', dual=False,\n",
    "                          fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, \n",
    "                          random_state=None, max_iter=100000)\n",
    "        \n",
    "        param_grid = {'linearsvc__penalty':['l2', 'l1'],'linearsvc__C': [0.001,0.01,0.1,1, 10,100,1000,10000,100000]}\n",
    "    \n",
    "    elif (name_model == 'gradientboosting'):\n",
    "        \n",
    "        # loss loss function to be optimized \n",
    "        # learning rate : Learning rate shrinks the contribution of each tree\n",
    "        # max depth : maximum depth of the individual regression estimators\n",
    "        # init : estimator object that is used to compute the initial predictions\n",
    "        # n_iter_no_change: early stop\n",
    "        \n",
    "        model = GradientBoostingClassifier(loss='deviance', n_estimators=100, subsample=1.0,\n",
    "                                           min_samples_split=2, min_samples_leaf=1, \n",
    "                                           min_weight_fraction_leaf=0.0,\n",
    "                                           min_impurity_decrease=0.0,n_iter_no_change=1,\n",
    "                                           random_state=None, max_features=None, verbose=0, \n",
    "                                           max_leaf_nodes=None, warm_start=False,\n",
    "                                           ccp_alpha=0.0)\n",
    "        \n",
    "        param_grid = {'gradientboostingclassifier__loss':['deviance', 'exponential'],\n",
    "                      'gradientboostingclassifier__learning_rate': [0.01,0.1,0.5],\n",
    "                      'gradientboostingclassifier__criterion':['friedman_mse', 'squared_error'],\n",
    "                      'gradientboostingclassifier__max_depth':[3],\n",
    "                     }\n",
    "        \n",
    "    elif (name_model == 'adaboost'):\n",
    "        \n",
    "        model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, \n",
    "                                   learning_rate=1.0, algorithm='SAMME', random_state=None)\n",
    "        \n",
    "        param_grid = {'adaboostclassifier__learning_rate':[0.1,0.5,1,2,5,10]}\n",
    "        \n",
    "    return(model, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f6494",
   "metadata": {},
   "source": [
    "# C-Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a02c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE= 40\n",
      "PERCENTAGE= 60\n",
      "PERCENTAGE= 80\n",
      "PERCENTAGE= 100\n",
      "7\r"
     ]
    }
   ],
   "source": [
    "# if I want the range detailed extended\n",
    "\n",
    "network = 'sample_following_fast'\n",
    "name_dataset = 'GiletsJaunes'\n",
    "N = 100000\n",
    "\n",
    "for percentage in [40, 60, 80, 100]:\n",
    "\n",
    "    with open(x_dir_data+'/df_experiment4/df_'+name_dataset+'_ad_premixed_'+network+'_N_'+str(N)+'_it_0_percentage_'+str(percentage)+'.pickle', 'rb') as handle:\n",
    "        df_general = pickle.load(handle)\n",
    "\n",
    "    df_general = df_general.dropna().reset_index(drop = True)\n",
    "\n",
    "    for it in [1,2,3,4,5,6,7]:\n",
    "\n",
    "        print(it, end = '\\r')\n",
    "\n",
    "        with open(x_dir_data+'/df_experiment4/df_'+name_dataset+'_ad_premixed_'+network+'_N_'+str(N)+'_it_'+str(it)+'_percentage_'+str(percentage)+'.pickle', 'rb') as handle:\n",
    "            df_local = pickle.load(handle)\n",
    "\n",
    "        df_local = df_local.dropna().reset_index(drop = True)\n",
    "        df_general = pd.concat([df_general, df_local])\n",
    "\n",
    "    exec('df_'+str(percentage)+' = df_general')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cf9ef",
   "metadata": {},
   "source": [
    "# Perform the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ae1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "name_model = 'random_forest'\n",
    "scale = 'StandardScaler()'\n",
    "nber_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca6373",
   "metadata": {},
   "source": [
    "### For the plots of Figure S6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb6663e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE 40\n",
      "0\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "1\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "2\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "3\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "4\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "5\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "6\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "7\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "8\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "9\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "PERCENTAGE 60\n",
      "0\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "1\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "2\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "3\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "4\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "5\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "6\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "7\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "8\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "9\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "PERCENTAGE 80\n",
      "0\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "1\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "2\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "3\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "4\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "5\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "6\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "7\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "8\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "9\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "PERCENTAGE 100\n",
      "0\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "1\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "2\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "3\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "4\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "5\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "6\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "7\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "8\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "9\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "for percentage in [40, 60, 80, 100]:\n",
    "    \n",
    "    print('PERCENTAGE', percentage)\n",
    "    \n",
    "    exec('df_general = df_'+str(percentage))\n",
    "\n",
    "    list_beta = list(df_general[df_general['type_contagion'] == 0]['beta'])\n",
    "    list_phi = list(df_general[df_general['type_contagion'] == 1]['phi'])\n",
    "\n",
    "    bins_beta = [np.percentile(list_beta, k*20) for k in range(1,6)]\n",
    "    bins_phi = [np.percentile(list_phi, k*20) for k in range(1,6)]\n",
    "\n",
    "    bins_beta_plot = [(a+b)/2 for a, b in zip(bins_beta[1:], bins_beta[:-1])]\n",
    "    bins_phi_plot = [(a+b)/2 for a, b in zip(bins_phi[1:], bins_phi[:-1])]\n",
    "    \n",
    "    ##############################################################################################\n",
    "    \n",
    "    L_score_beta = [[] for k in range(len(bins_beta)-1)]\n",
    "    L_score_phi = [[] for k in range(len(bins_phi)-1)]\n",
    "    L_score_r = []\n",
    "\n",
    "\n",
    "    for it in range(nber_iterations):\n",
    "\n",
    "        df = df_general.copy()\n",
    "\n",
    "        columns = ['degree',\n",
    "                   'nber_infected_neighbors',\n",
    "                   'prop_infected_neighbors',\n",
    "                   'sum_stimuli',\n",
    "                   'nber_stimuli_by_nei',\n",
    "                   'std_stimuli',\n",
    "                   'max_stimuli_one_nei',\n",
    "                   'nber_stim_last_infected_nei',\n",
    "                   'beta',\n",
    "                   'phi',\n",
    "                   'type_contagion']\n",
    "\n",
    "        df['type_contagion'] = np.where(df.is_seed == 2,2, df['type_contagion'])\n",
    "        df['type_contagion'] = np.where(df.is_seed == 1,2, df['type_contagion'])\n",
    "        df['type_contagion'] = df['type_contagion'].astype(int)\n",
    "\n",
    "        df = df[columns]\n",
    "        df_sample = df.sample(n=int(sample_size), replace=False, axis=0) \n",
    "        df_sample = df_sample.sample(frac = 1)\n",
    "\n",
    "        data = np.array(df_sample.loc[:, df_sample.columns != 'type_contagion'])\n",
    "        target = np.array(df_sample['type_contagion'])\n",
    "\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(data, target, train_size=0.75, random_state=42, stratify=target)\n",
    "\n",
    "        ytrain=ytrain.astype('int')\n",
    "        ytest=ytest.astype('int')\n",
    "\n",
    "        Xtrain = Xtrain[:,:-2] # remove beta and phi\n",
    "        betatest = Xtest[:,-2] # extract beta column\n",
    "        phitest = Xtest[:,-1] # extract phi column\n",
    "        Xtest = Xtest[:,:-2]\n",
    "\n",
    "        list_degree = Xtest[:,0]\n",
    "\n",
    "        model,param_grid = selection_model(name_model)\n",
    "\n",
    "        if scale == '':\n",
    "            pipe = make_pipeline(model) # \n",
    "        else:\n",
    "            exec('pipe = make_pipeline('+scale+', model)')\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv = 5, verbose=1)\n",
    "\n",
    "        grid.fit(Xtrain, ytrain)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        yfit = best_model.predict(Xtest)\n",
    "        \n",
    "        # save the model\n",
    "        with open(os.getcwd()+'/results/__model_ML_on_AD_GJ_percentage_'+str(percentage)+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(best_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        mat = confusion_matrix(ytest, yfit)\n",
    "        mat = np.rot90(mat, k=1, axes=(0, 1))\n",
    "        if len(mat) == 3:\n",
    "            score = (mat[2][0]+mat[1][1]+mat[0][2])/np.sum(mat)\n",
    "        else: \n",
    "            score = (mat[1][0]+mat[0][1])/np.sum(mat)\n",
    "\n",
    "        for index_beta in range(len(bins_beta)-1):\n",
    "            \n",
    "            min_bin = bins_beta[index_beta]\n",
    "            max_bin = bins_beta[index_beta+1]\n",
    "            ytest_local = ytest[np.where((ytest == 0)&(betatest >= min_bin)&(betatest < max_bin))[0]]\n",
    "            yfit_local = yfit[np.where((ytest == 0)&(betatest >= min_bin)&(betatest < max_bin))[0]]\n",
    "            score = np.sum(ytest_local == yfit_local)/len(yfit_local)\n",
    "            L_score_beta[index_beta].append(score)\n",
    "\n",
    "        for index_phi in range(len(bins_phi)-1):\n",
    "            \n",
    "            min_bin = bins_phi[index_phi]\n",
    "            max_bin = bins_phi[index_phi+1]\n",
    "            ytest_local = ytest[np.where((ytest == 1)&(phitest >= min_bin)&(phitest < max_bin))[0]]\n",
    "            yfit_local = yfit[np.where((ytest == 1)&(phitest >= min_bin)&(phitest < max_bin))[0]]\n",
    "            score = np.sum([a==b for a,b in zip(ytest_local, yfit_local)])/len(yfit_local)\n",
    "            L_score_phi[index_phi].append(score)\n",
    "\n",
    "        ytest_local = ytest[np.where((ytest == 2))[0]]\n",
    "        yfit_local = yfit[np.where((ytest == 2))[0]]\n",
    "        score = np.sum(ytest_local == yfit_local)/len(yfit_local)\n",
    "        L_score_r.append(score)\n",
    "        \n",
    "        nber_infected_neighbors = Xtest[:,2]\n",
    "        classification = ytest\n",
    "        \n",
    "    exec('L_score_beta_'+str(percentage)+' = L_score_beta')\n",
    "    exec('L_score_phi_'+str(percentage)+' = L_score_phi')\n",
    "    exec('L_score_r_'+str(percentage)+' = L_score_r')\n",
    "    exec('bins_beta_plot_'+str(percentage)+' = bins_beta_plot')\n",
    "    exec('bins_phi_plot_'+str(percentage)+' = bins_phi_plot')\n",
    "    \n",
    "    with open(os.getcwd()[:]+'/results/plot_score_beta_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_bootstrap_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(L_score_beta, handle)\n",
    "        \n",
    "    with open(os.getcwd()[:]+'/results/plot_score_phi_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_bootstrap_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(L_score_phi, handle)\n",
    "        \n",
    "    with open(os.getcwd()[:]+'/results/plot_score_r_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_bootstrap_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(L_score_r, handle)\n",
    "        \n",
    "    with open(os.getcwd()[:]+'/results/bins_beta_plot_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_bootstrap_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(bins_beta_plot, handle)\n",
    "        \n",
    "    with open(os.getcwd()[:]+'/results/bins_phi_plot_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_bootstrap_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(bins_phi_plot, handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a7384",
   "metadata": {},
   "source": [
    "### For the accuracy matrix of Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d7ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE 40\n",
      "0\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "1\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "2\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "3\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "4\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "for percentage in [40, 60, 80, 100]:\n",
    "    \n",
    "    exec('df_general = df_'+str(percentage))\n",
    "\n",
    "    list_beta = list(df_general[df_general['type_contagion'] == 0]['beta'])\n",
    "    list_phi = list(df_general[df_general['type_contagion'] == 1]['phi'])\n",
    "\n",
    "    bins_beta = [0]+[np.percentile(list_beta, k*20) for k in range(1,6)]\n",
    "    bins_phi = [0]+[np.percentile(list_phi, k*20) for k in range(1,6)]\n",
    "\n",
    "    bins_beta_plot = [(a+b)/2 for a, b in zip(bins_beta[1:], bins_beta[:-1])]\n",
    "    bins_phi_plot = [(a+b)/2 for a, b in zip(bins_phi[1:], bins_phi[:-1])]\n",
    "    \n",
    "    ##############################################################################################\n",
    "    \n",
    "    L_score_beta = [[] for k in range(len(bins_beta)-1)]\n",
    "    L_score_phi = [[] for k in range(len(bins_phi)-1)]\n",
    "    L_score_r = []\n",
    "    \n",
    "    mat_accuracies = [[[] for k in range(len(bins_phi)-1)] for i in range(len(bins_beta)-1)]\n",
    "\n",
    "    for it in range(nber_iterations):\n",
    "        print(it)\n",
    "\n",
    "        df = df_general.copy()\n",
    "\n",
    "        columns = ['degree',\n",
    "                   'nber_infected_neighbors',\n",
    "                   'prop_infected_neighbors',\n",
    "                   'sum_stimuli',\n",
    "                   'nber_stimuli_by_nei',\n",
    "                   'std_stimuli',\n",
    "                   'max_stimuli_one_nei',\n",
    "                   'nber_stim_last_infected_nei',\n",
    "                   'beta',\n",
    "                   'phi',\n",
    "                   'type_contagion']\n",
    "\n",
    "        df['type_contagion'] = np.where(df.is_seed == 2,2, df['type_contagion'])\n",
    "        df['type_contagion'] = np.where(df.is_seed == 1,2, df['type_contagion'])\n",
    "        df['type_contagion'] = df['type_contagion'].astype(int)\n",
    "\n",
    "        df = df[columns]\n",
    "        # I CHANGE HERE\n",
    "        df_sample = df.sample(n=int(sample_size), replace=False, axis=0) \n",
    "        df_sample = df_sample.sample(frac = 1)\n",
    "        #df_sample = df.copy()\n",
    "\n",
    "        data = np.array(df_sample.loc[:, df_sample.columns != 'type_contagion'])\n",
    "        target = np.array(df_sample['type_contagion'])\n",
    "\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(data, target, train_size=0.75, random_state=42, stratify=target)\n",
    "\n",
    "        ytrain=ytrain.astype('int')\n",
    "        ytest=ytest.astype('int')\n",
    "\n",
    "        Xtrain = Xtrain[:,:-2] # remove beta and phi\n",
    "        betatest = Xtest[:,-2] # extract beta column\n",
    "        phitest = Xtest[:,-1] # extract phi column\n",
    "        Xtest = Xtest[:,:-2]\n",
    "\n",
    "        list_degree = Xtest[:,0]\n",
    "\n",
    "        model,param_grid = selection_model(name_model)\n",
    "\n",
    "        if scale == '':\n",
    "            pipe = make_pipeline(model) # \n",
    "        else:\n",
    "            exec('pipe = make_pipeline('+scale+', model)')\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv = 5, verbose=1)\n",
    "\n",
    "        grid.fit(Xtrain, ytrain)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        yfit = best_model.predict(Xtest)\n",
    "        \n",
    "        # save the model\n",
    "        with open(os.getcwd()+'/results/__model_ML_on_AD_GJ_percentage_'+str(percentage)+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(best_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        for index_beta in range(len(bins_beta)-1):\n",
    "            min_bin_beta = bins_beta[index_beta]\n",
    "            max_bin_beta = bins_beta[index_beta+1]\n",
    "            \n",
    "            for index_phi in range(len(bins_phi)-1):\n",
    "                min_bin_phi = bins_phi[index_phi]\n",
    "                max_bin_phi = bins_phi[index_phi+1]\n",
    "                \n",
    "                ytest_local_beta = ytest[np.where((ytest == 0)&(betatest >= min_bin_beta)&(betatest < max_bin_beta))[0]]\n",
    "                ytest_local_phi = ytest[np.where((ytest == 1)&(phitest >= min_bin_phi)&(phitest < max_bin_phi))[0]]\n",
    "                ytest_local = np.concatenate((ytest_local_beta, ytest_local_phi), axis=None)\n",
    "                \n",
    "                yfit_local_beta = yfit[np.where((ytest == 0)&(betatest >= min_bin_beta)&(betatest < max_bin_beta))[0]]\n",
    "                yfit_local_phi = yfit[np.where((ytest == 1)&(phitest >= min_bin_phi)&(phitest < max_bin_phi))[0]]\n",
    "                yfit_local = np.concatenate((yfit_local_beta, yfit_local_phi), axis=None)\n",
    "\n",
    "                score = sum(ytest_local == yfit_local) / len(ytest_local)\n",
    "\n",
    "                mat_accuracies[index_beta][index_phi].append(score)\n",
    "\n",
    "    exec('mat_accuracies_'+str(percentage)+' = mat_accuracies')\n",
    "    \n",
    "    with open(os.getcwd()[:]+'/results/accuracies_ml_ad_percentage_'+str(percentage)+'_sample_size_'+str(sample_size)+'_nber_iterations_'+str(nber_iterations)+'_lognormal_distribution.pickle', 'wb') as handle:\n",
    "        pickle.dump(mat_accuracies, handle)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
