{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fc6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation of the packages\n",
    "\n",
    "import numpy as np\n",
    "from graph_tool.all import *\n",
    "import random\n",
    "import graph_tool.topology as gt\n",
    "import graph_tool.clustering as gc\n",
    "import graph_tool.centrality as gcent\n",
    "import graph_tool.generation as gg\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "import copy\n",
    "from matplotlib.lines import Line2D \n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from collections import Counter\n",
    "import math\n",
    "import scipy.stats\n",
    "import collections\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm \n",
    "from sklearn.neighbors import KernelDensity \n",
    "from sklearn.utils.fixes import parse_version \n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from collections import Counter\n",
    "#from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats.stats import pearsonr \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72584d8b",
   "metadata": {},
   "source": [
    "# A- getting the path of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea17e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dir_data = os.getcwd()[:-8]+'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1c249",
   "metadata": {},
   "source": [
    "# B-Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FUNCTIION 2\n",
    "'''\n",
    "\n",
    "def selection_model(name_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which takes in input the name of a machine learning algorithm and returns the model and the parameters for the grid search\n",
    "    \"\"\"\n",
    "    \n",
    "    if (name_model == 'SVM'):\n",
    "        \n",
    "        # C is the strength of the regularization\n",
    "        # kernel must be linear\n",
    "        # shrinking: shorten the training time if optimization problem solved/default is True\n",
    "        # probability: enable probability estimate, default is False\n",
    "        # tol: tolerance for stopping default=1e-3\n",
    "        # cache_size: size max before stopping default=200\n",
    "        # put weight on classes / default = False\n",
    "        # verbosebool: Enable verbose output - may conduct to errors/ default=False\n",
    "        # max_iter max of iteration, default=-1 meaning that infinite\n",
    "\n",
    "        model = SVC(kernel='linear')\n",
    "\n",
    "        # choose the best parameters\n",
    "\n",
    "        #Documentation on the C parameter:\n",
    "        #Parameter which \"soften\" the margin: that is, it allows some of the points to creep \n",
    "        #into the margin if that allows a better fit. The hardness of the margin is controlled \n",
    "        #by a tuning parameter, most often known as ùê∂. For very large ùê∂, the margin is hard, and \n",
    "        #points cannot lie in it. For smaller ùê∂, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "        #param_grid = {'svc__C': [0.01,0.1,1, 10,100,1000,10000]}\n",
    "        param_grid = {'C': [0.001,0.01,0.1,1, 10,100,1000,10000,100000]}\n",
    "        \n",
    "    elif (name_model == 'decision_tree'):\n",
    "        \n",
    "        # criterion is gini or entropy - Default = gini\n",
    "        # splitter split the best or the random best - ‚Äúbest‚Äù, ‚Äúrandom‚Äù - Default = \"best\"\n",
    "        # max_depth of the tree - Default=None (no limit)\n",
    "        # min_samples_split - minimum number of samples required to split an internal node - Default=2\n",
    "        # min_sample_leaf - minimum number of samples required to be at a leaf node - Default=1\n",
    "        # min_weight_fraction_leaf for weighted samples\n",
    "        # max_features - maximum number of features to take into account - Default = None (-all the features)\n",
    "        # random_state\n",
    "        # max_leaf_node - number of leaf nodes - Default = None (no limit)\n",
    "        # min_impurity_decrease - split induces a decrease of the impurity greater than or equal to this value - Default = 0\n",
    "        # class_weight - for weighted data\n",
    "        # ccp_alpha - The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen - Default = 0\n",
    "        \n",
    "        model = DecisionTreeClassifier(splitter='best', max_depth=None, \n",
    "                                       min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                       random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       class_weight=None, ccp_alpha=0.0)\n",
    "        \n",
    "        param_grid = {'decisiontreeclassifier__criterion':['gini', 'entropy']}\n",
    "        \n",
    "    elif (name_model == 'random_forest'):\n",
    "        \n",
    "        # n_estimators - number of trees - default is 100\n",
    "        # bootstrap - if we use all the dataset to do a tree - default = True\n",
    "        # oob_score - if bootstrap = True\n",
    "        # n_jobs - run in parallele\n",
    "        # warm_start - use the paraemter of the last run as a start\n",
    "        # max_samples - if bootstrap is true\n",
    "        \n",
    "        model = RandomForestClassifier()#n_estimators=100, max_depth=None, \n",
    "                                       #min_samples_split=2, min_samples_leaf=1, \n",
    "                                       #min_weight_fraction_leaf=0.0, #   max_features=None, \n",
    "                                       #max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                       #oob_score=False, # bootstrap=False, \n",
    "                                       #n_jobs=None, random_state=None,                                     \n",
    "                                       #verbose=0, warm_start=False, \n",
    "                                       #class_weight=None, ccp_alpha=0.0, \n",
    "                                       #max_samples=None)\n",
    "        \n",
    "        param_grid = {'randomforestclassifier__criterion':['gini', 'entropy']}\n",
    "        \n",
    "    elif (name_model == 'knn'):\n",
    "        \n",
    "        # n_neighbors - Number of neighbors to considere - Default = 5\n",
    "        # weights 'uniform' or 'distance' - weight points in function of the distance or uniformly - Default = uniform\n",
    "        # kind of used algo - Default = 'auto' - try to decide the best\n",
    "        # leaf_size - to BallTree or KDTree - about the speed of the construction - Default = 30\n",
    "        # p - Power parameter for the Minkowski metric - p=2 is euclidean distance\n",
    "        # metric - of the distance 'minkowski'\n",
    "        # metric_params-Additional keyword arguments for the metric function\n",
    "        # n_jobs - The number of parallel jobs to run for neighbors search. None means 1\n",
    "        \n",
    "        model = KNeighborsClassifier(n_neighbors=5, weights='uniform', n_jobs=1)\n",
    "        \n",
    "        param_grid = {'kneighborsclassifier__n_neighbors':[5,10,15,20],'kneighborsclassifier__weights':['uniform','distance']}\n",
    "        \n",
    "    elif (name_model == 'naive_bayes'):\n",
    "        \n",
    "        # priors - Prior probabilities of the classes. If specified the priors are not adjusted according to the data.Default = None\n",
    "        # var_smoothing Portion of the largest variance of all features \n",
    "        #that is added to variances for calculation stability - Default = 1e-9\n",
    "        \n",
    "        model = GaussianNB(priors=None, var_smoothing=1e-9)\n",
    "        \n",
    "        param_grid = {'gaussiannb__var_smoothing':[1e-10,1e-9,1e-8]}\n",
    "        \n",
    "    elif (name_model == 'perceptron'):\n",
    "        \n",
    "        # penalty: gives a penalty to avoid overfitting in {‚Äòl2‚Äô,‚Äôl1‚Äô,‚Äôelasticnet‚Äô}, default=None\n",
    "        # alpha - cst which multiplies the penalty: default=0.0001\n",
    "        # l1_ratio -only for penalty='elasticnet' - Default=0.15\n",
    "        # fit_interceptbool, default=True - Whether the intercept should be estimated or not.If False, the data is assumed to be already centered.\n",
    "        # max_iterint, maximum number of passes over the training data (aka epochs) - default=1000\n",
    "        # tol float, default=1e-3 Stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).\n",
    "        # shufflebool, default=True - Whether or not the training data should be shuffled after each epoch.\n",
    "        # verboseint, default=0 - The verbosity level.\n",
    "        # eta0 double, default=1 Constant by which the updates are multiplied.\n",
    "        # n_jobsint, default=None - The number of CPUs to use to do the OVA (One Versus All)\n",
    "        # random_stateint, RandomState instance, default=None\n",
    "        # early_stopping bool, default=False-Whether to use early stopping to terminate training when validation\n",
    "        # validation_fraction float, default=0.1 The proportion of training data to set aside as validation set for early stopping.\n",
    "        # n_iter_no_change int, default=5 - Number of iterations with no improvement to wait before early stopping.\n",
    "        # class_weightdict, {class_label: weight} or ‚Äúbalanced‚Äù, default=None - Preset for the class_weight fit parameter.\n",
    "            #Weights associated with classes. If not given, all classes are supposed to have weight one.\n",
    "            #The ‚Äúbalanced‚Äù mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "        # warm_startbool, default=False - when set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.\n",
    "\n",
    "        \n",
    "        model = Perceptron(penalty=None, max_iter = 20000)\n",
    "        \n",
    "        param_grid = {'perceptron__penalty':[None, 'l2', 'l1', 'elasticnet'],'perceptron__alpha':[0.00001,0.0001,0.001],'perceptron__eta0':[0.5,1,2,3,4,5]}\n",
    "        \n",
    "    elif (name_model == 'linear_svm'):\n",
    "        \n",
    "        \n",
    "        #penalty{‚Äòl1‚Äô, ‚Äòl2‚Äô}, default=‚Äôl2‚Äô - Specifies the norm used in the penalization.\n",
    "        #loss{‚Äòhinge‚Äô, ‚Äòsquared_hinge‚Äô}, default=‚Äôsquared_hinge‚Äô-Specifies the loss function. \n",
    "        #‚Äòhinge‚Äô is the standard SVM loss (used e.g. by the SVC class) while ‚Äòsquared_hinge‚Äô is the square of the hinge loss. The combination of penalty='l1' and loss='hinge' is not supported.\n",
    "        # dualbool, default=True - Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "        # tolfloat, default=1e-4 - Tolerance for stopping criteria. - Cfloat, default=1.0\n",
    "        # C: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "        # multi_class{‚Äòovr‚Äô, ‚Äòcrammer_singer‚Äô}, default=‚Äôovr‚Äô - Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored.\n",
    "        # fit_interceptbool, default=True - To center the data. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered).\n",
    "        # intercept_scaling float, default=1 - When self.fit_intercept is True, \n",
    "            #instance vector x becomes [x, self.intercept_scaling], i.e. a ‚Äúsynthetic‚Äù feature with constant value\n",
    "            #equals to intercept_scaling is appended to the instance vector. \n",
    "            #The intercept becomes intercept_scaling * synthetic feature weight \n",
    "            #Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. \n",
    "            #To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept)\n",
    "            #intercept_scaling has to be increased.\n",
    "\n",
    "        # class_weightdict or ‚Äòbalanced‚Äô, default=None - Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. \n",
    "        # The ‚Äúbalanced‚Äù mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "        # verboseint, default=0 - Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.\n",
    "        # random_stateint, RandomState instance or None, default=None - Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if dual=True). When dual=False the underlying implementation of LinearSVC is not random and random_state has no effect on the results. Pass an int for reproducible output across multiple function calls. See Glossary.\n",
    "        # max_iterint, default=1000 - The maximum number of iterations to be run.\n",
    "\n",
    "\n",
    "        \n",
    "        model = LinearSVC(loss='squared_hinge', dual=False,\n",
    "                          fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, \n",
    "                          random_state=None, max_iter=100000)\n",
    "        \n",
    "        param_grid = {'linearsvc__penalty':['l2', 'l1'],'linearsvc__C': [0.001,0.01,0.1,1, 10,100,1000,10000,100000]}\n",
    "    \n",
    "    elif (name_model == 'gradientboosting'):\n",
    "        \n",
    "        # loss loss function to be optimized \n",
    "        # learning rate : Learning rate shrinks the contribution of each tree\n",
    "        # max depth : maximum depth of the individual regression estimators\n",
    "        # init : estimator object that is used to compute the initial predictions\n",
    "        # n_iter_no_change: early stop\n",
    "        \n",
    "        model = GradientBoostingClassifier(loss='deviance', n_estimators=100, subsample=1.0,\n",
    "                                           min_samples_split=2, min_samples_leaf=1, \n",
    "                                           min_weight_fraction_leaf=0.0,\n",
    "                                           min_impurity_decrease=0.0,n_iter_no_change=1,\n",
    "                                           random_state=None, max_features=None, verbose=0, \n",
    "                                           max_leaf_nodes=None, warm_start=False,\n",
    "                                           ccp_alpha=0.0)\n",
    "        \n",
    "        param_grid = {'gradientboostingclassifier__loss':['deviance', 'exponential'],\n",
    "                      'gradientboostingclassifier__learning_rate': [0.01,0.1,0.5],\n",
    "                      'gradientboostingclassifier__criterion':['friedman_mse', 'squared_error'],\n",
    "                      'gradientboostingclassifier__max_depth':[3],\n",
    "                     }\n",
    "        \n",
    "    elif (name_model == 'adaboost'):\n",
    "        \n",
    "        model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, \n",
    "                                   learning_rate=1.0, algorithm='SAMME', random_state=None)\n",
    "        \n",
    "        param_grid = {'adaboostclassifier__learning_rate':[0.1,0.5,1,2,5,10]}\n",
    "        \n",
    "    return(model, param_grid)\n",
    "\n",
    "def powerset(iterable):\n",
    "    return chain.from_iterable(list(combinations(iterable, r)) for r in range(1,len(iterable)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d540df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_beta_phi(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function which takes in input:\n",
    "    - network: type of network used\n",
    "    - name_model: machine learning algorithm name\n",
    "    - sample size: the size of the sample\n",
    "    - beta: parameter of the simple contagion\n",
    "    - phi: parameter of the complex contagion\n",
    "    - nber_sample: number of samples\n",
    "    and return the confusion matrix and the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['degree','nber_infected_neighbors','sum_stimuli',\n",
    "               'time_last_infected_neighbor','std_stimuli','nber_stimuli_by_neighbors',\n",
    "               'prop_infected_neighbors','time_first_infected_neighbor','parameter','contagion']\n",
    "    \n",
    "    network,name_model,sample_size,beta,phi,nber_sample = x\n",
    "    \n",
    "    rate = 0.005\n",
    "    \n",
    "    df_general = pd.read_pickle(x_dir+'/df_experiment2_3/df_'+network+'_premix_different_parameter.pickle') \n",
    "\n",
    "    df_SI = df_general[(df_general['contagion'] == 0) & (df_general['parameter'] == beta)]\n",
    "    df_CP = df_general[(df_general['contagion'] == 1) & (df_general['parameter'] == phi)]\n",
    "\n",
    "    df_SI['contagion'] = np.where(df_SI.is_seed == 2,2, df_SI['contagion'])\n",
    "    df_CP['contagion'] = np.where(df_CP.is_seed == 2,2, df_CP['contagion'])\n",
    "    \n",
    "    df_SI['contagion'] = np.where(df_SI.is_seed == 1,2, df_SI['contagion'])\n",
    "    df_CP['contagion'] = np.where(df_CP.is_seed == 1,2, df_CP['contagion'])\n",
    "    \n",
    "    df_SI = df_SI[columns]\n",
    "    df_CP = df_CP[columns]\n",
    "    \n",
    "    df_random = pd.concat([df_SI[df_SI['contagion'] == 2],df_CP[df_CP['contagion'] == 2]])\n",
    "    df_SI = df_SI[df_SI['contagion'] == 0]\n",
    "    df_CP = df_CP[df_CP['contagion'] == 1]\n",
    "    \n",
    "    df_sample_SI = df_SI.sample(n=int(sample_size*0.25), replace=True)\n",
    "    df_sample_CP = df_CP.sample(n=int(sample_size*0.25), replace=True)\n",
    "    df_sample_random = df_random.sample(n=int(sample_size*0.25), replace=True)\n",
    "\n",
    "    df_sample = pd.concat([df_sample_SI,df_sample_CP,df_sample_random])\n",
    "\n",
    "    df_sample = df_sample.sample(frac = 1)\n",
    "\n",
    "    data = np.array(df_sample.loc[:, df_sample.columns != 'contagion'])\n",
    "    target = np.array(df_sample['contagion'])\n",
    "    \n",
    "    Xtest = data.copy()\n",
    "    ytest = target.copy()\n",
    "\n",
    "    # extracting the parameter column\n",
    "    Xparamtrain = Xtrain[:,-1]\n",
    "    Xparamtest = Xtest[:,-1]\n",
    "\n",
    "    # removing the parameter column\n",
    "    Xtrain = Xtrain[:,:-1]\n",
    "    Xtest = Xtest[:,:-1]\n",
    "\n",
    "    # move ytrain in the correct type\n",
    "    ytrain=ytrain.astype('int')\n",
    "    ytest=ytest.astype('int')\n",
    "\n",
    "    # definition of the model\n",
    "\n",
    "    model,param_grid = selection_model(name_model)\n",
    "\n",
    "    # scoring can choose how to evaluate the performance\n",
    "    # n_jobs number of job to run in parallel, default is None, meaning 1\n",
    "    # refitbool: Refit an estimator using the best found parameters on the whole dataset default=True\n",
    "    # cv number to use in the k-fold estimator. Default is 5\n",
    "    # verbose : Controls the verbosity: the higher, the more messages.\n",
    "    # pre dispatch: number of jobs that get dispatched during parallel execution default 2*n_job\n",
    "    # error_score: Value to assign to the score if an error occurs in estimator fitting Default: NaN\n",
    "    # return_train_score Default = False\n",
    "\n",
    "\n",
    "    # make the pipeline\n",
    "    pipe = make_pipeline(StandardScaler(),model) # \n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = 5, verbose=1)\n",
    "\n",
    "    # fit of the model\n",
    "    grid.fit(Xtrain, ytrain)\n",
    "\n",
    "    # prediction on the test dataset:\n",
    "    best_model = grid.best_estimator_\n",
    "    yfit = best_model.predict(Xtest)\n",
    "\n",
    "    # making confusion matrix - The confusion matrix is saved in the correct order for the plot\n",
    "    mat = confusion_matrix(ytest, yfit)\n",
    "    \n",
    "    # get the score\n",
    "    score = (mat[0][0]+mat[1][1]+mat[2][2])/np.sum(mat)\n",
    "    \n",
    "    with open(os.getcwd()+'/results/data_beta_'+str(beta)+'_phi_'+str(phi)+'_premix_different_parameters.pickle', \"wb\") as f:\n",
    "        pickle.dump([mat,score,np.nan], f)\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    with open(os.getcwd()+'/results/best_model_ml_'+network+'_beta_'+str(beta)+'_phi_'+str(phi)+'_premix_different_parameters.pickle', \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "        \n",
    "    f.close()\n",
    "        \n",
    "    return(mat, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a672768",
   "metadata": {},
   "source": [
    "# C-making the classification on the whole dataset of Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc86aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nber_sample 0 / 1\n",
      "beta, phi=beta, phi= beta, phi=0.1 beta, phi=beta, phi=beta, phi=beta, phi= beta, phi=beta, phi=0.1 \n",
      "   beta, phi=  0.30.1 0.1  0.30.3 0.1\n",
      " 0.90.1 0.3  0.10.3 0.5\n",
      "\n",
      " 0.7\n",
      "0.70.30.90.3 \n",
      "\n",
      "\n",
      "\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "name_model = 'random_forest'\n",
    "only_cascade = False\n",
    "sample_size = 8000\n",
    "nber_sample = 10\n",
    "\n",
    "for network in ['ER','BA','WS','SBM','twitter']:\n",
    "\n",
    "    # '''\n",
    "    # CONCEPTION OF THE INPUT LIST\n",
    "    # '''\n",
    "\n",
    "    list_beta = [0.1,0.3,0.5,0.7,0.9]\n",
    "    list_phi = [0.1,0.3,0.5,0.7,0.9]\n",
    "\n",
    "    list_inputs = []\n",
    "\n",
    "    for beta in list_beta:\n",
    "        for phi in list_phi:\n",
    "            list_inputs.append((network,name_model,sample_size,beta,phi,nber_sample))\n",
    "\n",
    "    # '''\n",
    "    # CONCEPTION OF THE SUBSETS\n",
    "    # '''\n",
    "\n",
    "    list_optional_features = ['degree', \n",
    "                              'nber_infected_neighbors',\n",
    "                              'sum_stimuli',\n",
    "                              'time_last_infected_neighbor',\n",
    "                              'std_stimuli',\n",
    "                              'nber_stimuli_by_neighbors',\n",
    "                              'prop_infected_neighbors',\n",
    "                              'time_first_infected_neighbor',\n",
    "                              'parameter',\n",
    "                              'contagion'\n",
    "                             ]\n",
    "\n",
    "    list_score_matrices = []\n",
    "    list_confusion_matrices = []\n",
    "\n",
    "    for sample in range(nber_sample):\n",
    "\n",
    "        print('nber_sample',sample,'/',nber_sample)\n",
    "\n",
    "        pool = Pool(processes = 10)\n",
    "        pool.map(classification_beta_phi,list_inputs)\n",
    "        \n",
    "        print('out')\n",
    "\n",
    "        # reconstruction of the matrices\n",
    "\n",
    "        # definition of the matrices full of 0s.\n",
    "        confusion_matrices = np.asarray([[np.array([[0,0,0],[0,0,0],[0,0,0]]) for k in range(len(list_phi))] for i in range(len(list_beta))])\n",
    "        scores_matrix = np.zeros((len(list_beta),len(list_phi)))\n",
    "        #features_matrix  = np.asarray([[tuple(0 for k in range(len(subset))) for k in range(len(list_phi))] for i in range(len(list_beta))])\n",
    "\n",
    "        # fill up the matrices\n",
    "        for index_beta, beta in enumerate(list_beta):\n",
    "            for index_phi, phi in enumerate(list_phi):\n",
    "\n",
    "                # load of the data\n",
    "\n",
    "                with open(os.getcwd()+'/results/data_beta_'+str(beta)+'_phi_'+str(phi)+'_premix_different_parameters.pickle', 'rb') as handle:\n",
    "                    [mat,score,classification_features] = pickle.load(handle)\n",
    "\n",
    "                confusion_matrices[index_beta][index_phi] = mat\n",
    "                scores_matrix[index_beta][index_phi] = score\n",
    "\n",
    "        list_score_matrices.append(scores_matrix)\n",
    "        list_confusion_matrices.append(confusion_matrices)\n",
    "\n",
    "    # saving of the training set\n",
    "\n",
    "    with open('_ml_experiment2_confusion_matrices_'+network+'_'+name_model+'_'+str(sample_size)+'_premix_different_parameter.pickle', 'wb') as handle:\n",
    "        pickle.dump(list_confusion_matrices, handle)\n",
    "\n",
    "    # saving of the training set\n",
    "\n",
    "    with open('_ml_experiment2_scores_matrix_'+network+'_'+name_model+'_'+str(sample_size)+'_premix_different_parameter.pickle', 'wb') as handle:\n",
    "        pickle.dump(list_score_matrices, handle)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
